{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "style-transfer.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgxQsiv89Wh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "plt.rcParams['figure.figsize'] = (14.0, 10.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn3NK7ee9WiL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87a88875-f087-40ce-e757-3e8a61d01a89"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nm9XhtD9WiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://pytorch.org/docs/stable/torchvision/models.html\n",
        "#normalize with provided mean and std for pretrained models\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "def img2tensor(img):\n",
        "    normalize = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "    tfms = transforms.Compose([\n",
        "        transforms.Resize(512),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "    \n",
        "    img = tfms(img)\n",
        "    img = img.unsqueeze(0)\n",
        "    \n",
        "    return img\n",
        "\n",
        "def tensor2img(tensor):\n",
        "    img = tensor.clone().detach().cpu().numpy().squeeze().transpose(1, 2, 0)\n",
        "    img = img * np.array(std) + np.array(mean)\n",
        "      \n",
        "    return img\n",
        "\n",
        "content_img = Image.open('ana.jpg').convert('RGB') \n",
        "style_img = Image.open('ana.jpg').convert('RGB') \n",
        "\n",
        "style_tensor = img2tensor(style_img)\n",
        "content_tensor = img2tensor(content_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM3U_7DR9Wih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(style_img)\n",
        "display(content_img)\n",
        "display(style_img.size, content_img.size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glkN0SM9Wip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.vgg19(pretrained = True).features\n",
        "\n",
        "for p in model.parameters():\n",
        "    p.requires_grad_(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG3LLtse9Wi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "style_tensor = style_tensor.to(device)\n",
        "content_tensor = content_tensor.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYhhbDUz9WjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(tensor2img(style_tensor))\n",
        "plt.figure()\n",
        "plt.imshow(tensor2img(content_tensor))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ujsPZq99WjQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b639293e-84a6-4173-c861-5ef5f84c2e63"
      },
      "source": [
        "style_layers = [0, 5, 10, 19, 28]\n",
        "style_weights = [1, 0.75, 0.5, 0.35, 0.25, 0.15]\n",
        "content_layers = [21]\n",
        "\n",
        "layers = sorted(style_layers + content_layers)\n",
        "display(layers)\n",
        "\n",
        "\n",
        "def extract_features(x, model):\n",
        "    features = {}\n",
        "\n",
        "    for i, (name, layer) in enumerate(model._modules.items()):\n",
        "        x = layer(x)\n",
        "        if i in layers:\n",
        "            features[i] = x\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def calc_gram_matrix(tensor):\n",
        "    _, channels, height, width = tensor.size()\n",
        "    tensor = tensor.view(channels, height * width)\n",
        "    gram_m = torch.mm(tensor, tensor.t())\n",
        "    gram_m = gram_m.div(channels * width * height)\n",
        "    \n",
        "    return gram_m\n",
        "    \n",
        "style_ftrs = extract_features(style_tensor, model)\n",
        "style_ftrs_gram_m = { layer: calc_gram_matrix(style_ftrs[layer]) for layer in style_ftrs }\n",
        "content_ftrs = extract_features(content_tensor, model)\n",
        "\n",
        "\n",
        "display(style_ftrs_gram_m[0].dtype)\n",
        "display(content_ftrs.keys())"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[0, 5, 10, 19, 21, 28]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dict_keys([0, 5, 10, 19, 21, 28])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AwSovl99WjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYeYYLbk9Wjc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94ef67c0-4cde-4b02-cb99-6ec61691be06"
      },
      "source": [
        "epochs = 500\n",
        "optimizer = optim.Adam([target], lr=1e-2)\n",
        "style_loss_weight = 1e6\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    target_ftrs = extract_features(target, model)\n",
        "    \n",
        "    content_loss = 0\n",
        "    for l in content_layers:\n",
        "        # m = torch.sin(torch.arange(content_ftrs[l].numel(), dtype=torch.float64).view(content_ftrs[l].shape) + (epoch/epochs) * np.pi).to(device)\n",
        "        # content_ftrs[l] = content_ftrs[l] * m\n",
        "        content_loss += F.mse_loss(target_ftrs[l], content_ftrs[l])\n",
        "        \n",
        "    style_loss = 0\n",
        "    for l, w in zip(style_layers, style_weights):\n",
        "        target_ftrs_gram_m = { layer: calc_gram_matrix(target_ftrs[layer]) for layer in style_ftrs }\n",
        "        style_loss += F.mse_loss(target_ftrs_gram_m[l], style_ftrs_gram_m[l]) * w\n",
        "        \n",
        "        \n",
        "    total_loss = content_loss + style_loss * style_loss_weight\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        " \n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch {epoch}', style_loss.item(), style_loss.item() * style_loss_weight, content_loss.item())\n",
        " \n",
        "    if epoch % 100 == 0:\n",
        "      img = tensor2img(target)\n",
        "      images.append(img)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 0.0 0.0 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEEoBF0h9Wjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    plt.figure()\n",
        "    plt.imshow(tensor2img(target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoDrQQjl9Wjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}