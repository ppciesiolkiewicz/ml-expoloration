{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "plt.rcParams['figure.figsize'] = (14.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/torchvision/models.html\n",
    "#normalize with provided mean and std for pretrained models\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def img2tensor(img):\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    tfms = transforms.Compose([\n",
    "        transforms.Resize(512),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    img = tfms(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def tensor2img(tensor):\n",
    "    img = tensor.clone().detach().cpu().numpy().squeeze().transpose(1, 2, 0)\n",
    "    img *= np.array(std) + np.array(mean)\n",
    "    img = img.clip(0, 1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "content_img = Image.open('cat.jpg').convert('RGB') \n",
    "style_img = Image.open('ana.jpg').convert('RGB') \n",
    "\n",
    "style_tensor = img2tensor(style_img)\n",
    "content_tensor = img2tensor(content_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(style_img)\n",
    "display(content_img)\n",
    "display(style_img.size, content_img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg19(pretrained = True).features\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "style_tensor = style_tensor.to(device)\n",
    "content_tensor = content_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(tensor2img(style_tensor))\n",
    "plt.figure()\n",
    "plt.imshow(tensor2img(content_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = [0, 5, 10, 19, 28]\n",
    "style_weights = [1, 0.75, 0.5, 0.35, 0.25, 0.15]\n",
    "content_layers = [21]\n",
    "\n",
    "layers = sorted(style_layers + content_layers)\n",
    "display(layers)\n",
    "\n",
    "\n",
    "def extract_features(x, model):\n",
    "    features = {}\n",
    "\n",
    "    for i, (name, layer) in enumerate(model._modules.items()):\n",
    "        x = layer(x)\n",
    "        if i in layers:\n",
    "            features[i] = x\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def calc_gram_matrix(tensor):\n",
    "    _, channels, height, width = tensor.size()\n",
    "    tensor = tensor.view(channels, height * width)\n",
    "    gram_m = torch.mm(tensor, tensor.t())\n",
    "    gram_m = gram_m.div(channels * width * height)\n",
    "    \n",
    "    return gram_m\n",
    "    \n",
    "style_ftrs = extract_features(style_tensor, model)\n",
    "style_ftrs_gram_m = { layer: calc_gram_matrix(style_ftrs[layer]) for layer in style_ftrs }\n",
    "content_ftrs = extract_features(content_tensor, model)\n",
    "\n",
    "\n",
    "display(style_ftrs_gram_m.keys())\n",
    "display(content_ftrs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = torch.randn(content_tensor.shape).requires_grad_(True).to(device)\n",
    "target = content_tensor.clone().detach().cpu().requires_grad_(True).to(device)\n",
    "\n",
    "plt.imshow(tensor2img(style_tensor))\n",
    "plt.imshow(tensor2img(content_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([target], lr=7e-3)\n",
    "style_loss_weight = 1e6\n",
    "\n",
    "\n",
    "for epoch in range(300):\n",
    "    target_ftrs = extract_features(target, model)\n",
    "    \n",
    "    content_loss = 0\n",
    "    for l in content_layers:\n",
    "        content_loss += F.mse_loss(target_ftrs[l], content_ftrs[l])\n",
    "        \n",
    "    style_loss = 0\n",
    "    for l, w in zip(style_layers, style_weights):\n",
    "        target_ftrs_gram_m = { layer: calc_gram_matrix(target_ftrs[layer]) for layer in style_ftrs }\n",
    "        style_loss += F.mse_loss(target_ftrs_gram_m[l], style_ftrs_gram_m[l]) * w\n",
    "        \n",
    "        \n",
    "    total_loss = content_loss + style_loss * style_loss_weight\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch}', style_loss.item(), style_loss.item() * style_loss_weight, content_loss.item())\n",
    " \n",
    "    if epoch % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            plt.figure()\n",
    "            plt.imshow(tensor2img(target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "    plt.imshow(tensor2img(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
