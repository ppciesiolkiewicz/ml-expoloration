{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "def calc_acc(model, dataset):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for xb,yb in dataset:\n",
    "        yb_hat = model(xb)\n",
    "        yb_hat = torch.argmax(yb_hat, dim=-1)\n",
    "\n",
    "        correct += (yb == yb_hat).sum().item()\n",
    "        total += yb_hat.size()[0]\n",
    "                \n",
    "    return round(correct/total*100, 3), correct, total\n",
    "\n",
    "print('starting accuracy', calc_acc(model, testset))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb,yb in testset:\n",
    "        model.zero_grad()\n",
    "        output = model(xb)\n",
    "        loss = F.nll_loss(output, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} --\")\n",
    "    print(loss.item())\n",
    "    print(calc_acc(model, testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb,yb in testset:\n",
    "        x = xb[IDX]\n",
    "        y = yb[IDX]\n",
    "        y_hat = model(x)\n",
    "        y_hat = torch.argmax(y_hat)\n",
    "\n",
    "        plt.imshow(xb[IDX].view(28, 28))\n",
    "        plt.title(f\"y_hat={y_hat}, y={y}\")\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.handlers.tensorboard_logger import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "epochs = 3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3f)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "metrics = {\"accuracy\": Accuracy(), \"loss\": Loss(criterion)}\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "validation_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def compute_metrics(engine):\n",
    "    train_evaluator.run(trainset)\n",
    "    validation_evaluator.run(testset)\n",
    "\n",
    "tb_logger = TensorboardLogger(log_dir='/home/asdf/tensorboard')\n",
    "\n",
    "tb_logger.attach(\n",
    "    trainer,\n",
    "    log_handler=OutputHandler(\n",
    "        tag=\"training\", output_transform=lambda loss: {\"batchloss\": loss}, metric_names=\"all\"\n",
    "    ),\n",
    "    event_name=Events.ITERATION_COMPLETED(every=100),\n",
    ")\n",
    "\n",
    "tb_logger.attach(\n",
    "    train_evaluator,\n",
    "    log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\", \"accuracy\"], another_engine=trainer),\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    ")\n",
    "\n",
    "tb_logger.attach(\n",
    "    validation_evaluator,\n",
    "    log_handler=OutputHandler(tag=\"validation\", metric_names=[\"loss\", \"accuracy\"], another_engine=trainer),\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    ")\n",
    "\n",
    "tb_logger.attach(\n",
    "    trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_COMPLETED(every=100)\n",
    ")\n",
    "\n",
    "tb_logger.attach(trainer, log_handler=WeightsScalarHandler(model), event_name=Events.ITERATION_COMPLETED(every=100))\n",
    "\n",
    "tb_logger.attach(trainer, log_handler=WeightsHistHandler(model), event_name=Events.EPOCH_COMPLETED(every=100))\n",
    "\n",
    "tb_logger.attach(trainer, log_handler=GradsScalarHandler(model), event_name=Events.ITERATION_COMPLETED(every=100))\n",
    "\n",
    "tb_logger.attach(trainer, log_handler=GradsHistHandler(model), event_name=Events.EPOCH_COMPLETED(every=100))\n",
    "\n",
    "trainer.run(trainset, max_epochs=epochs)\n",
    "tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
