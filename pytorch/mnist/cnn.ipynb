{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "   transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train = datasets.MNIST('', train=True, download=True, transform=transform)\n",
    "test = datasets.MNIST('', train=False, download=True, transform=transform)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2)\n",
    "        \n",
    "        # forward pass of feature extractor to calculate input size of fc1\n",
    "        x = self.feature_extractor(torch.randn((1, *input_shape)))\n",
    "        self.to_linear_shape =  x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "\n",
    "        self.fc1 = nn.Linear(self.to_linear_shape, 128)   \n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        for n, l in self.named_parameters():\n",
    "            if l.dim() == 1:\n",
    "                nn.init.constant_(l, 0.01)\n",
    "            else:\n",
    "                nn.init.kaiming_uniform_(l, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "\n",
    "    def feature_extractor(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "                \n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(-1, self.to_linear_shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=-1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = CNN((1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = F.nll_loss\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "def calc_acc(model, dataset):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for xb,yb in dataset:\n",
    "            yb_hat = model(xb)\n",
    "            yb_hat = torch.argmax(yb_hat, dim=-1)\n",
    "\n",
    "            correct += (yb == yb_hat).sum().item()\n",
    "            total += yb_hat.size()[0]\n",
    "\n",
    "        model.train()\n",
    "        return round(correct/total*100, 3), correct, total\n",
    "\n",
    "\n",
    "print('starting accuracy', calc_acc(model, testset))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (xb,yb) in enumerate(trainset):\n",
    "        model.zero_grad()\n",
    "        output = model(xb)\n",
    "        loss = loss_fn(output, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print('loss: ', loss.item())\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch} --\")\n",
    "    print('%/correct/total', calc_acc(model, testset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 1\n",
    "with torch.no_grad():\n",
    "    for xb,yb in testset:\n",
    "        x = xb[IDX]\n",
    "        y = yb[IDX]\n",
    "        y_hat = model(x.unsqueeze(0))\n",
    "        y_hat = torch.argmax(y_hat)\n",
    "\n",
    "        plt.imshow(xb[IDX].view(28, 28))\n",
    "        plt.title(f\"y_hat={y_hat}, y={y}\")\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler, ParamGroupScheduler\n",
    "from ignite.contrib.handlers.tensorboard_logger import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epochs = 3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "lr_scheduler = CosineAnnealingScheduler(optimizer, \"lr\", start_value=1e-3, end_value=1e-5, cycle_size=20,start_value_mult=0.7)\n",
    "schedulers = [lr_scheduler]\n",
    "schedulers_names = [\"lr\"]\n",
    "scheduler = ParamGroupScheduler(schedulers=schedulers, names=schedulers_names)\n",
    "\n",
    "\n",
    "\n",
    "metrics = {\"accuracy\": Accuracy(), \"loss\": Loss(criterion)}\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "validation_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def compute_metrics(engine):\n",
    "    train_evaluator.run(trainset)\n",
    "    validation_evaluator.run(testset)\n",
    "    \n",
    "def compute_metrics(engine):\n",
    "    train_evaluator.run(trainset)\n",
    "    validation_evaluator.run(testset)\n",
    "\n",
    "tb_logger = TensorboardLogger(log_dir='/home/asdf/tensorboard')\n",
    "\n",
    "tb_logger.attach(\n",
    "    trainer,\n",
    "    log_handler=OutputHandler(\n",
    "        tag=\"training\", output_transform=lambda loss: {\"batchloss\": loss}, metric_names=\"all\"\n",
    "    ),\n",
    "    event_name=Events.ITERATION_COMPLETED(every=100),\n",
    ")\n",
    "\n",
    "tb_logger.attach(\n",
    "    train_evaluator,\n",
    "    log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\", \"accuracy\"], another_engine=trainer),\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    ")\n",
    "\n",
    "tb_logger.attach(\n",
    "    validation_evaluator,\n",
    "    log_handler=OutputHandler(tag=\"validation\", metric_names=[\"loss\", \"accuracy\"], another_engine=trainer),\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    ")\n",
    "\n",
    "tb_logger.attach(\n",
    "    trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_COMPLETED(every=100)\n",
    ")\n",
    "\n",
    "tb_logger.attach(trainer, log_handler=WeightsScalarHandler(model), event_name=Events.ITERATION_COMPLETED(every=100))\n",
    "\n",
    "tb_logger.attach(trainer, log_handler=WeightsHistHandler(model), event_name=Events.EPOCH_COMPLETED(every=100))\n",
    "\n",
    "tb_logger.attach(trainer, log_handler=GradsScalarHandler(model), event_name=Events.ITERATION_COMPLETED(every=100))\n",
    "\n",
    "tb_logger.attach(trainer, log_handler=GradsHistHandler(model), event_name=Events.EPOCH_COMPLETED(every=100))\n",
    "\n",
    "trainer.run(trainset, max_epochs=epochs)\n",
    "tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
